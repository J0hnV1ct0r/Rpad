{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2       3        4        5        6        7       8  \\\n",
       "0    17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710  0.2419   \n",
       "1    20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017  0.1812   \n",
       "2    19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790  0.2069   \n",
       "3    11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520  0.2597   \n",
       "4    20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430  0.1809   \n",
       "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
       "564  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890  0.1726   \n",
       "565  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791  0.1752   \n",
       "566  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302  0.1590   \n",
       "567  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200  0.2397   \n",
       "568   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000  0.1587   \n",
       "\n",
       "           9  ...     21      22      23       24       25      26      27  \\\n",
       "0    0.07871  ...  17.33  184.60  2019.0  0.16220  0.66560  0.7119  0.2654   \n",
       "1    0.05667  ...  23.41  158.80  1956.0  0.12380  0.18660  0.2416  0.1860   \n",
       "2    0.05999  ...  25.53  152.50  1709.0  0.14440  0.42450  0.4504  0.2430   \n",
       "3    0.09744  ...  26.50   98.87   567.7  0.20980  0.86630  0.6869  0.2575   \n",
       "4    0.05883  ...  16.67  152.20  1575.0  0.13740  0.20500  0.4000  0.1625   \n",
       "..       ...  ...    ...     ...     ...      ...      ...     ...     ...   \n",
       "564  0.05623  ...  26.40  166.10  2027.0  0.14100  0.21130  0.4107  0.2216   \n",
       "565  0.05533  ...  38.25  155.00  1731.0  0.11660  0.19220  0.3215  0.1628   \n",
       "566  0.05648  ...  34.12  126.70  1124.0  0.11390  0.30940  0.3403  0.1418   \n",
       "567  0.07016  ...  39.42  184.60  1821.0  0.16500  0.86810  0.9387  0.2650   \n",
       "568  0.05884  ...  30.37   59.16   268.6  0.08996  0.06444  0.0000  0.0000   \n",
       "\n",
       "         28       29    y  \n",
       "0    0.4601  0.11890  0.0  \n",
       "1    0.2750  0.08902  0.0  \n",
       "2    0.3613  0.08758  0.0  \n",
       "3    0.6638  0.17300  0.0  \n",
       "4    0.2364  0.07678  0.0  \n",
       "..      ...      ...  ...  \n",
       "564  0.2060  0.07115  0.0  \n",
       "565  0.2572  0.06637  0.0  \n",
       "566  0.2218  0.07820  0.0  \n",
       "567  0.4087  0.12400  0.0  \n",
       "568  0.2871  0.07039  1.0  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast=pd.read_csv('dados/breast.csv',header=None)\n",
    "breast=breast.rename(columns={breast.columns[30]:'y'})\n",
    "breast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nomalizer:\n",
    "    def __init__(self):\n",
    "        self.y_max=0.0\n",
    "        self.y_min=0.0\n",
    "        self.x_max=[]\n",
    "        self.x_min=[]\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        # Pega os valores minimos de \n",
    "        self.y_max=y['y'].max().tolist()\n",
    "        self.y_min=y['y'].min().tolist()\n",
    "\n",
    "        # Pega os valores maximos e minimos do X\n",
    "        self.x_max=x.max().tolist()\n",
    "        self.x_min=x.min().tolist()\n",
    "    \n",
    "    def normalize(self,x,y):\n",
    "        y_norm=y.map(self.y_norm_aux)\n",
    "        x_norm=x.copy()\n",
    "        norm_index=0\n",
    "        for column in x.columns:\n",
    "            x_column=[]\n",
    "            for i in range(len(x[column])):\n",
    "                x_column.append((x[column][i]-self.x_min[norm_index])/(self.x_max[norm_index]-self.x_min[norm_index]))\n",
    "                \n",
    "            norm_index+=1\n",
    "            x_norm[column]=x_column    \n",
    "        \n",
    "        return x_norm,y_norm\n",
    "    \n",
    "    def desnormalize(self,x_norm,y_norm):\n",
    "        y=y_norm.map(self.y_desnorm_aux)\n",
    "        x=x_norm.copy()\n",
    "        norm_index=0\n",
    "        for column in x_norm.columns:\n",
    "            x_column=[]\n",
    "            for i in range(len(x_norm[column])):\n",
    "                x_column.append((x_norm[column][i]*(self.x_max[norm_index]-self.x_min[norm_index]))+self.x_min[norm_index])\n",
    "                \n",
    "            norm_index+=1\n",
    "            x[column]=x_column    \n",
    "        \n",
    "        return x,y\n",
    "    \n",
    "    def y_norm_aux(self,y):\n",
    "        return (y-self.y_min)/(self.y_max-self.y_min)\n",
    "    \n",
    "    def y_desnorm_aux(self,y):\n",
    "        return (y*(self.y_max-self.y_min))+self.y_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier:\n",
    "  def __init__(self,opt='SGD'):\n",
    "    self.w =[]\n",
    "    self.losses=[]\n",
    "    \n",
    "\n",
    "  def fit(self, X, y, n, reg, lr=0.001, epochs=100):\n",
    "    x_poli = self.polinomial_transfomation(X,n)# Transforma em polinomio\n",
    "    df_bias = pd.DataFrame({'bias':[1]*len(x_poli)})\n",
    "    X_bias=pd.concat([df_bias,x_poli], axis=1)\n",
    "    self.w=np.ones(X_bias.shape[1])\n",
    "    self.fit_SGD(X_bias,y,reg,lr,epochs)\n",
    "\n",
    "  def polinomial_transfomation(self,x,n):\n",
    "    df_columns = x.columns\n",
    "    poli_columns={}\n",
    "    self.n=n\n",
    "    x_poli=x.copy()\n",
    "    for column in df_columns:\n",
    "      for i in range(2,n+1):\n",
    "        poli_columns[f\"{column}_{i}\"] = x[column]**i\n",
    "    poli_df = pd.DataFrame(poli_columns)\n",
    "    x_poli = pd.concat([x,poli_df], axis=1)\n",
    "    return x_poli\n",
    "\n",
    "\n",
    "  def fit_SGD(self, X, y, reg, lr, epochs):\n",
    "    for i in range(epochs):\n",
    "      errors=[]\n",
    "      indices = list(range(len(X)))\n",
    "      random.shuffle(indices)\n",
    "\n",
    "      for sample in indices:\n",
    "        #Calculo dos erros\n",
    "        y_pred=(X.iloc[sample]*self.w).sum()# esta certo\n",
    "        y_sig=1 / (1 + np.exp(-y_pred))# esta certo\n",
    "        error_np=y.iloc[sample]-y_sig# esta certo\n",
    "        error=error_np.item()#esta certo\n",
    "        errors.append(error)\n",
    "        \n",
    "        #ajuste dos Ws    \n",
    "        w_arr = np.array(self.w)#esta certo\n",
    "        inter = lr*error*X.iloc[sample]#esta certo\n",
    "        w_novo = (reg*w_arr) + inter#esta certo\n",
    "        self.w=w_novo\n",
    "        \n",
    "        #Calculo da loss\n",
    "        self.loss_function(X, y, reg)\n",
    "        \n",
    "      \n",
    "  def sigma(self,x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "      \n",
    "  def loss_function(self, x, y,reg):\n",
    "    w=np.array(self.w)\n",
    "    x_arr=x.to_numpy()\n",
    "    y_arr=y.to_numpy()\n",
    "\n",
    "    N = len(y_arr)#Numero de amostras\n",
    "    p = np.dot(x_arr, w)#Produto interno para cada xi\n",
    "    sigma = 1/(1+np.exp(-p))\n",
    "    epsilon = 1e-15# sujeira\n",
    "    sigma = np.clip(sigma, epsilon, 1 - epsilon)# Garante que não ocorra log(0)\n",
    "    loss = -(1/N)*np.sum(y_arr*np.log(sigma)+(1-y_arr)*np.log(1-sigma)) #Calcula a perda e salva na lista\n",
    "    self.losses.append(loss)\n",
    "    \n",
    "\n",
    "  def predict(self, X):\n",
    "    y=0\n",
    "    y_pred=[]# Lista para salvar as predições\n",
    "    df_bias = pd.DataFrame({'bias':[1]*len(X)})\n",
    "    x_poli = self.polinomial_transfomation(X,self.n)\n",
    "    X_bias=pd.concat([df_bias,x_poli], axis=1)\n",
    "    for sample in range(len(X_bias)):\n",
    "      y_sigma=self.sigma((X_bias.iloc[sample]*self.w).sum())# Calcula a predição \n",
    "      y_pred.append(0 if y_sigma<=0.5 else 1)# Aplica a função degrau para definir a classe e salva na lista\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=breast.iloc[:, 0:30]  \n",
    "y=breast[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=Nomalizer()\n",
    "norm.fit(x,y)\n",
    "norm_Trx,norm_Try=norm.normalize(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521037</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.545989</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.792037</td>\n",
       "      <td>0.703140</td>\n",
       "      <td>0.731113</td>\n",
       "      <td>0.686364</td>\n",
       "      <td>0.605518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620776</td>\n",
       "      <td>0.141525</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.450698</td>\n",
       "      <td>0.601136</td>\n",
       "      <td>0.619292</td>\n",
       "      <td>0.568610</td>\n",
       "      <td>0.912027</td>\n",
       "      <td>0.598462</td>\n",
       "      <td>0.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643144</td>\n",
       "      <td>0.272574</td>\n",
       "      <td>0.615783</td>\n",
       "      <td>0.501591</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.181768</td>\n",
       "      <td>0.203608</td>\n",
       "      <td>0.348757</td>\n",
       "      <td>0.379798</td>\n",
       "      <td>0.141323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606901</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.539818</td>\n",
       "      <td>0.435214</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>0.192971</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.233590</td>\n",
       "      <td>0.222878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.601496</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.449417</td>\n",
       "      <td>0.514309</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.462512</td>\n",
       "      <td>0.635686</td>\n",
       "      <td>0.509596</td>\n",
       "      <td>0.211247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556386</td>\n",
       "      <td>0.360075</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.483590</td>\n",
       "      <td>0.385375</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.403706</td>\n",
       "      <td>0.213433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210090</td>\n",
       "      <td>0.360839</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>0.102906</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.811361</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.522863</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.385928</td>\n",
       "      <td>0.241347</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>0.548642</td>\n",
       "      <td>0.884880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.630986</td>\n",
       "      <td>0.489290</td>\n",
       "      <td>0.430351</td>\n",
       "      <td>0.347893</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.518390</td>\n",
       "      <td>0.378283</td>\n",
       "      <td>0.186816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519744</td>\n",
       "      <td>0.123934</td>\n",
       "      <td>0.506948</td>\n",
       "      <td>0.341575</td>\n",
       "      <td>0.437364</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.319489</td>\n",
       "      <td>0.558419</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.142595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.428813</td>\n",
       "      <td>0.678668</td>\n",
       "      <td>0.566490</td>\n",
       "      <td>0.526948</td>\n",
       "      <td>0.296055</td>\n",
       "      <td>0.571462</td>\n",
       "      <td>0.690358</td>\n",
       "      <td>0.336364</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623266</td>\n",
       "      <td>0.383262</td>\n",
       "      <td>0.576174</td>\n",
       "      <td>0.452664</td>\n",
       "      <td>0.461137</td>\n",
       "      <td>0.178527</td>\n",
       "      <td>0.328035</td>\n",
       "      <td>0.761512</td>\n",
       "      <td>0.097575</td>\n",
       "      <td>0.105667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.622320</td>\n",
       "      <td>0.626987</td>\n",
       "      <td>0.604036</td>\n",
       "      <td>0.474019</td>\n",
       "      <td>0.407782</td>\n",
       "      <td>0.257714</td>\n",
       "      <td>0.337395</td>\n",
       "      <td>0.486630</td>\n",
       "      <td>0.349495</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.699094</td>\n",
       "      <td>0.520892</td>\n",
       "      <td>0.379915</td>\n",
       "      <td>0.300007</td>\n",
       "      <td>0.159997</td>\n",
       "      <td>0.256789</td>\n",
       "      <td>0.559450</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>0.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.455251</td>\n",
       "      <td>0.621238</td>\n",
       "      <td>0.445788</td>\n",
       "      <td>0.303118</td>\n",
       "      <td>0.288165</td>\n",
       "      <td>0.254340</td>\n",
       "      <td>0.216753</td>\n",
       "      <td>0.263519</td>\n",
       "      <td>0.267677</td>\n",
       "      <td>0.137321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393099</td>\n",
       "      <td>0.589019</td>\n",
       "      <td>0.379949</td>\n",
       "      <td>0.230731</td>\n",
       "      <td>0.282177</td>\n",
       "      <td>0.273705</td>\n",
       "      <td>0.271805</td>\n",
       "      <td>0.487285</td>\n",
       "      <td>0.128721</td>\n",
       "      <td>0.151909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.644564</td>\n",
       "      <td>0.663510</td>\n",
       "      <td>0.665538</td>\n",
       "      <td>0.475716</td>\n",
       "      <td>0.588336</td>\n",
       "      <td>0.790197</td>\n",
       "      <td>0.823336</td>\n",
       "      <td>0.755467</td>\n",
       "      <td>0.675253</td>\n",
       "      <td>0.425442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633582</td>\n",
       "      <td>0.730277</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.402035</td>\n",
       "      <td>0.619626</td>\n",
       "      <td>0.815758</td>\n",
       "      <td>0.749760</td>\n",
       "      <td>0.910653</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.452315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.036869</td>\n",
       "      <td>0.501522</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266162</td>\n",
       "      <td>0.187026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054287</td>\n",
       "      <td>0.489072</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257441</td>\n",
       "      <td>0.100682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.521037  0.022658  0.545989  0.363733  0.593753  0.792037  0.703140   \n",
       "1    0.643144  0.272574  0.615783  0.501591  0.289880  0.181768  0.203608   \n",
       "2    0.601496  0.390260  0.595743  0.449417  0.514309  0.431017  0.462512   \n",
       "3    0.210090  0.360839  0.233501  0.102906  0.811321  0.811361  0.565604   \n",
       "4    0.629893  0.156578  0.630986  0.489290  0.430351  0.347893  0.463918   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "564  0.690000  0.428813  0.678668  0.566490  0.526948  0.296055  0.571462   \n",
       "565  0.622320  0.626987  0.604036  0.474019  0.407782  0.257714  0.337395   \n",
       "566  0.455251  0.621238  0.445788  0.303118  0.288165  0.254340  0.216753   \n",
       "567  0.644564  0.663510  0.665538  0.475716  0.588336  0.790197  0.823336   \n",
       "568  0.036869  0.501522  0.028540  0.015907  0.000000  0.074351  0.000000   \n",
       "\n",
       "           7         8         9   ...        20        21        22  \\\n",
       "0    0.731113  0.686364  0.605518  ...  0.620776  0.141525  0.668310   \n",
       "1    0.348757  0.379798  0.141323  ...  0.606901  0.303571  0.539818   \n",
       "2    0.635686  0.509596  0.211247  ...  0.556386  0.360075  0.508442   \n",
       "3    0.522863  0.776263  1.000000  ...  0.248310  0.385928  0.241347   \n",
       "4    0.518390  0.378283  0.186816  ...  0.519744  0.123934  0.506948   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "564  0.690358  0.336364  0.132056  ...  0.623266  0.383262  0.576174   \n",
       "565  0.486630  0.349495  0.113100  ...  0.560655  0.699094  0.520892   \n",
       "566  0.263519  0.267677  0.137321  ...  0.393099  0.589019  0.379949   \n",
       "567  0.755467  0.675253  0.425442  ...  0.633582  0.730277  0.668310   \n",
       "568  0.000000  0.266162  0.187026  ...  0.054287  0.489072  0.043578   \n",
       "\n",
       "           23        24        25        26        27        28        29  \n",
       "0    0.450698  0.601136  0.619292  0.568610  0.912027  0.598462  0.418864  \n",
       "1    0.435214  0.347553  0.154563  0.192971  0.639175  0.233590  0.222878  \n",
       "2    0.374508  0.483590  0.385375  0.359744  0.835052  0.403706  0.213433  \n",
       "3    0.094008  0.915472  0.814012  0.548642  0.884880  1.000000  0.773711  \n",
       "4    0.341575  0.437364  0.172415  0.319489  0.558419  0.157500  0.142595  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "564  0.452664  0.461137  0.178527  0.328035  0.761512  0.097575  0.105667  \n",
       "565  0.379915  0.300007  0.159997  0.256789  0.559450  0.198502  0.074315  \n",
       "566  0.230731  0.282177  0.273705  0.271805  0.487285  0.128721  0.151909  \n",
       "567  0.402035  0.619626  0.815758  0.749760  0.910653  0.497142  0.452315  \n",
       "568  0.020497  0.124084  0.036043  0.000000  0.000000  0.257441  0.100682  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_Trx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////////////////////////////////////////////\n",
      "acuracia do modelo:  60.43859649122807\n",
      "revocação do modelo:  0.6228070175438597\n",
      "precisao do modelo:  0.6228070175438597\n",
      "f1-score do modelo:  0.6228070175438597\n",
      "/////////////////////////////////////////////////\n",
      "acuracia do modelo:  63.666666666666664\n",
      "revocação do modelo:  0.6666666666666666\n",
      "precisao do modelo:  0.6754385964912281\n",
      "f1-score do modelo:  0.6710239651416122\n",
      "/////////////////////////////////////////////////\n",
      "acuracia do modelo:  61.175438596491226\n",
      "revocação do modelo:  0.6491228070175439\n",
      "precisao do modelo:  0.6228070175438597\n",
      "f1-score do modelo:  0.6356926799758015\n",
      "/////////////////////////////////////////////////\n",
      "acuracia do modelo:  61.175438596491226\n",
      "revocação do modelo:  0.6491228070175439\n",
      "precisao do modelo:  0.6228070175438597\n",
      "f1-score do modelo:  0.6356926799758015\n",
      "/////////////////////////////////////////////////\n",
      "acuracia do modelo:  60.86725663716814\n",
      "revocação do modelo:  0.7079646017699115\n",
      "precisao do modelo:  0.5929203539823009\n",
      "f1-score do modelo:  0.6453554873276744\n",
      "/////////////////////////////////////////////////\n",
      "Acuracia médio: 61.464679397609075\n",
      "Acuracia em cada fold: [60.43859649122807, 63.666666666666664, 61.175438596491226, 61.175438596491226, 60.86725663716814]\n",
      "Revocação médio: 0.6591367800031052\n",
      "Revocações em cada fold: [0.6228070175438597, 0.6666666666666666, 0.6491228070175439, 0.6491228070175439, 0.7079646017699115]\n",
      "Precisão médio: 0.6273560006210216\n",
      "Precisões em cada fold: [0.6228070175438597, 0.6754385964912281, 0.6228070175438597, 0.6228070175438597, 0.5929203539823009]\n",
      "F1-score médio: 0.6421143659929499\n",
      "F1-scores em cada fold: [0.6228070175438597, 0.6710239651416122, 0.6356926799758015, 0.6356926799758015, 0.6453554873276744]\n",
      "/////////////////////////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "# Configurando o KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Para armazenar métricas de cada fold\n",
    "mse_scores = []\n",
    "acuracias =[]\n",
    "revocacoes=[]\n",
    "precisoes=[]\n",
    "f1_scores=[]\n",
    "\n",
    "# Loop pelos folds\n",
    "for train_index, test_index in kf.split(x):\n",
    "    # Dividindo os dados em treino e teste\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Normalizando os dados\n",
    "    norm=Nomalizer()\n",
    "    norm.fit(X_train,y_train)\n",
    "    norm_Trx,norm_Try=norm.normalize(X_train,y_train)\n",
    "    norm_Tsx,norm_Tsy=norm.normalize(X_test,y_test)\n",
    "\n",
    "\n",
    "    # Treinando um modelo\n",
    "    model = LogisticClassifier()\n",
    "    model.fit(norm_Trx,norm_Try,n=1, reg=1, lr=0.001, epochs=50)\n",
    "\n",
    "    # Fazendo previsões\n",
    "    y_pred = model.predict(norm_Tsx)\n",
    "    print(\"/////////////////////////////////////////////////\")\n",
    "    # Calculo da acuracia\n",
    "    acetertos = np.sum(np.array(y_pred)==norm_Tsy.to_numpy())\n",
    "    acuracia = acetertos / len(norm_Tsy)\n",
    "    print(\"acuracia do modelo: \",acuracia)\n",
    "    acuracias.append(acuracia)\n",
    "\n",
    "    # Calculo da revocação\n",
    "    TP = np.sum((np.array(y_pred) == 1) & (norm_Tsy.to_numpy() == 1))  # Previsões corretas para a classe positiva\n",
    "    FN = np.sum((np.array(y_pred) == 0) & (norm_Tsy.to_numpy() == 1))  # Valores reais positivos, mas previstos como negativos\n",
    "    revocacao = TP / (TP + FN)\n",
    "    revocacoes.append(revocacao)\n",
    "    print(\"revocação do modelo: \",revocacao)\n",
    "\n",
    "    # Calculando a precisão\n",
    "    VP = np.sum((norm_Tsy.to_numpy() == 1) & (np.array(y_pred) == 1))\n",
    "    FP = np.sum((norm_Tsy.to_numpy() == 0) & (np.array(y_pred) == 1))\n",
    "    precisao = VP / (VP + FP) \n",
    "    precisoes.append(precisao)\n",
    "    print(\"precisao do modelo: \",precisao)\n",
    "\n",
    "    # Calculando f1-score\n",
    "    f1_score = 2 * (precisao * revocacao) / (precisao + revocacao)\n",
    "    f1_scores.append(f1_score)\n",
    "    print(\"f1-score do modelo: \",f1_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(\"/////////////////////////////////////////////////\")\n",
    "print(\"Acuracia médio:\", np.mean(acuracias))\n",
    "print(\"Acuracia em cada fold:\", acuracias)\n",
    "\n",
    "print(\"Revocação médio:\", np.mean(revocacoes))\n",
    "print(\"Revocações em cada fold:\", revocacoes)\n",
    "\n",
    "print(\"Precisão médio:\", np.mean(precisoes))\n",
    "print(\"Precisões em cada fold:\", precisoes)\n",
    "\n",
    "print(\"F1-score médio:\", np.mean(f1_scores))\n",
    "print(\"F1-scores em cada fold:\", f1_scores)\n",
    "print(\"/////////////////////////////////////////////////\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17543859649122806\n",
      "0.21052631578947367\n",
      "0.20175438596491227\n",
      "0.15789473684210525\n",
      "0.17699115044247787\n",
      "MSE médio: 0.18452103710603943\n",
      "MSE em cada fold: [0.17543859649122806, 0.21052631578947367, 0.20175438596491227, 0.15789473684210525, 0.17699115044247787]\n"
     ]
    }
   ],
   "source": [
    "# Configurando o KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Para armazenar métricas de cada fold\n",
    "mse_scores = []\n",
    "\n",
    "# Loop pelos folds\n",
    "for train_index, test_index in kf.split(x):\n",
    "    # Dividindo os dados em treino e teste\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Normalizando os dados\n",
    "    norm=Nomalizer()\n",
    "    norm.fit(X_train,y_train)\n",
    "    norm_Trx,norm_Try=norm.normalize(X_train,y_train)\n",
    "    norm_Tsx,norm_Tsy=norm.normalize(X_test,y_test)\n",
    "\n",
    "\n",
    "    # Treinando um modelo\n",
    "    model = LogisticClassifier()\n",
    "    model.fit(norm_Trx,norm_Try,n=2, reg=1, lr=0.001, epochs=50)\n",
    "\n",
    "    # Fazendo previsões\n",
    "    y_pred = model.predict(norm_Tsx)\n",
    "\n",
    "    # Calculando a métrica (MSE neste caso)\n",
    "    mse = mean_squared_error(norm_Tsy, y_pred)\n",
    "    print(mse)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(\"MSE médio:\", np.mean(mse_scores))\n",
    "print(\"MSE em cada fold:\", mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3684210526315789\n",
      "0.37719298245614036\n",
      "0.41228070175438597\n",
      "0.2894736842105263\n",
      "0.34513274336283184\n",
      "MSE médio: 0.35850023288309274\n",
      "MSE em cada fold: [0.3684210526315789, 0.37719298245614036, 0.41228070175438597, 0.2894736842105263, 0.34513274336283184]\n"
     ]
    }
   ],
   "source": [
    "# Configurando o KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Para armazenar métricas de cada fold\n",
    "mse_scores = []\n",
    "\n",
    "# Loop pelos folds\n",
    "for train_index, test_index in kf.split(x):\n",
    "    # Dividindo os dados em treino e teste\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Normalizando os dados\n",
    "    norm=Nomalizer()\n",
    "    norm.fit(X_train,y_train)\n",
    "    norm_Trx,norm_Try=norm.normalize(X_train,y_train)\n",
    "    norm_Tsx,norm_Tsy=norm.normalize(X_test,y_test)\n",
    "\n",
    "\n",
    "    # Treinando um modelo\n",
    "    model = LogisticClassifier()\n",
    "    model.fit(norm_Trx,norm_Try,n=9, reg=1, lr=0.001, epochs=50)\n",
    "\n",
    "    # Fazendo previsões\n",
    "    y_pred = model.predict(norm_Tsx)\n",
    "\n",
    "    # Calculando a métrica (MSE neste caso)\n",
    "    mse = mean_squared_error(norm_Tsy, y_pred)\n",
    "    print(mse)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(\"MSE médio:\", np.mean(mse_scores))\n",
    "print(\"MSE em cada fold:\", mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticClassifier()\n",
    "lc.fit(norm_Trx,norm_Try,n=3, reg=1, lr=0.001, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.predict(norm_Trx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc2 = LogisticClassifier()\n",
    "lc2.fit(norm_Trx,norm_Try,n=9, reg=1, lr=0.001, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2745198595142778,\n",
       " -2.5760181003767837,\n",
       " -3.461830586937145,\n",
       " 22.243964297467457,\n",
       " -2.088091680949746,\n",
       " 0.7370190412499196,\n",
       " -2.992796583121917,\n",
       " 0.533314444762212,\n",
       " -0.028741162008451088,\n",
       " 16.232338300876748,\n",
       " -0.5635042747848502,\n",
       " -1.784981560176302,\n",
       " 5.252120979685005,\n",
       " -0.3983142215807758,\n",
       " 0.5273344867983742,\n",
       " 1.0041100502692861,\n",
       " -1.429521496152323,\n",
       " -1.26592682453211,\n",
       " -3.4225509856391056,\n",
       " -0.5859463172548072,\n",
       " 0.29576779685645527,\n",
       " 1.571947335272902,\n",
       " -0.8655454206016833,\n",
       " -2.0590255061538443,\n",
       " -1.9123344873795536,\n",
       " 2.886686602464323,\n",
       " -2.495690519336417,\n",
       " -1.322931042172105,\n",
       " -1.5932103260092823,\n",
       " -1.796065484029672,\n",
       " -1.8514295719752945,\n",
       " 1.760600167810931,\n",
       " -1.4340782896780606,\n",
       " -1.833309908255477,\n",
       " -1.794332960252547,\n",
       " -1.3798937650223566,\n",
       " -0.8962696984227025,\n",
       " 0.9797814000255591,\n",
       " 1.3483029566069402,\n",
       " -2.4186500111549227,\n",
       " -0.5701676608855488,\n",
       " 1.3602710705784884,\n",
       " 1.6626096718667185,\n",
       " -0.7983832053442157,\n",
       " -1.3379612297476833,\n",
       " -3.089962567300655,\n",
       " 1.8597909076513788,\n",
       " -0.8824129525777005,\n",
       " 0.160419379884043,\n",
       " -0.530523007947966,\n",
       " 0.943485648598466,\n",
       " -0.22203212297176028,\n",
       " 0.6618174640807541,\n",
       " -1.2344638094194837,\n",
       " -1.441552331639622,\n",
       " 1.098902525297169,\n",
       " -2.5435635143388295,\n",
       " -1.3629988667045345,\n",
       " 1.0222894056710732,\n",
       " 2.3387714455267568,\n",
       " 4.724584414049478,\n",
       " 3.457143394639111,\n",
       " -0.19461326084688713,\n",
       " 3.095126482240469,\n",
       " -0.08803131766065153,\n",
       " -0.7671728245528454,\n",
       " 2.265830959715741,\n",
       " 0.5740076628042953,\n",
       " 11.25788076801863,\n",
       " 0.44291076551757824,\n",
       " -2.7700088269014684,\n",
       " 8.33817240994712,\n",
       " -0.9088379326189033,\n",
       " -1.1214909092140826,\n",
       " -0.012058418718486966,\n",
       " -1.412080271512046,\n",
       " 2.053687708888165,\n",
       " -1.6544657348539147,\n",
       " 18.243376947348253,\n",
       " 0.13747592771330278,\n",
       " 1.9415487473721957,\n",
       " -0.8389280347340742,\n",
       " 2.6012531646724484,\n",
       " 0.1628019413109344,\n",
       " 0.6313804898936116,\n",
       " -1.659986165433663,\n",
       " 0.011101076960492118,\n",
       " -2.7343101891298955,\n",
       " 0.19355713240383798,\n",
       " -0.5576601267947887,\n",
       " 0.06910308143583391,\n",
       " -1.1964567836933604,\n",
       " -0.6475238653638468,\n",
       " 0.08709020087922145,\n",
       " -2.540717165683092,\n",
       " -1.5534517272825887,\n",
       " 1.108799204518263,\n",
       " 2.340207321324975,\n",
       " 0.23632507325049615,\n",
       " -0.8187825363396232,\n",
       " -0.4456335144370467,\n",
       " 3.7920656887242252,\n",
       " 0.5011552426281273,\n",
       " 0.8189872440134821,\n",
       " 1.9210436154755484,\n",
       " 1.09634602510207,\n",
       " 0.7603748980057867,\n",
       " -0.0593948432740382,\n",
       " 7.754710727450338,\n",
       " 0.8382624420881697,\n",
       " 2.293362990577876,\n",
       " 0.9098034477070107,\n",
       " 0.931007535789069,\n",
       " 2.591203349246793,\n",
       " 1.950291639801704,\n",
       " 0.6076956045189814,\n",
       " 3.0514517022335754,\n",
       " -1.1914910968176362,\n",
       " -0.6171991825091423,\n",
       " 0.47611901945678536,\n",
       " 0.2018264139436376,\n",
       " -1.7120645805322878,\n",
       " 16.098063367859517,\n",
       " -0.7267605935495689,\n",
       " -0.40893482460409447,\n",
       " 0.06432335234137447,\n",
       " -0.4662770456662454,\n",
       " -1.4743715047283983,\n",
       " -0.5161166285818873,\n",
       " -2.246440013572898,\n",
       " 0.8420316254065328,\n",
       " -1.810293533706255,\n",
       " -1.0819240431503307,\n",
       " -1.4635127055172164,\n",
       " -1.759627178404628,\n",
       " 0.3644867779837565,\n",
       " 1.1130659334715176,\n",
       " 0.15978857572857005,\n",
       " 0.7786541625226087,\n",
       " 1.0098991492396614,\n",
       " 1.7677844629804296,\n",
       " -1.011304340402021,\n",
       " 1.3138481268804623,\n",
       " -0.19463846341726387,\n",
       " 0.8271521152727028,\n",
       " 2.5446590923867825,\n",
       " 4.373088656021512,\n",
       " 0.6687440233536793,\n",
       " -1.546509723761941,\n",
       " -0.23623418113251854,\n",
       " 2.0202300518552043,\n",
       " 4.579702617325088,\n",
       " 34.12665454812408,\n",
       " 1.0771754285593884,\n",
       " 0.22319237718009222,\n",
       " 0.33251732082623947,\n",
       " -1.4448553337006818,\n",
       " -0.4602000301486082,\n",
       " 0.3037279819033714,\n",
       " 0.8111024734254753,\n",
       " 1.3890857565247379,\n",
       " -1.8563608680956287,\n",
       " -3.2038637096541973,\n",
       " 1.1169963602363446,\n",
       " -2.455713092285174,\n",
       " -0.14437267518404892,\n",
       " 0.8409919948386406,\n",
       " -1.5160234341460401,\n",
       " -2.0044890464380445,\n",
       " -0.39296679026173054,\n",
       " 0.28525007364865185,\n",
       " -0.6628328380706524,\n",
       " -1.653207625342979,\n",
       " 2.251231355367302,\n",
       " 2.051121425293976,\n",
       " 2.1443040620423286,\n",
       " 5.098342250922613,\n",
       " -2.5268551820703133,\n",
       " 0.6682570567291823,\n",
       " 0.7159676649384138,\n",
       " 6.781806090793928,\n",
       " -0.8675562405434138,\n",
       " -1.5271046339229695,\n",
       " 0.7924073260713148,\n",
       " -1.044924163091736,\n",
       " 2.208485382561098,\n",
       " -2.57855234052129,\n",
       " 0.0031108365128985765,\n",
       " 1.4557446692737344,\n",
       " 0.2876586608527808,\n",
       " 13.232008474435977,\n",
       " 1.3539868504863881,\n",
       " 10.148181673335687,\n",
       " 1.593623996875479,\n",
       " -1.367721641455028,\n",
       " 0.0006221524374563775,\n",
       " 0.8403685266430644,\n",
       " -0.8027615328620928,\n",
       " -2.263094506554082,\n",
       " -1.3691415219134073,\n",
       " 0.12898760512579277,\n",
       " -2.963480490954264,\n",
       " -0.49557141028022045,\n",
       " 5.492405245073203,\n",
       " 0.013450766513700259,\n",
       " -1.14287493517455,\n",
       " 1.6297392892286138,\n",
       " -1.0082264285834517,\n",
       " 0.43651016900384987,\n",
       " -1.2551876328631348,\n",
       " -2.051877484213726,\n",
       " 0.3673521850560087,\n",
       " 30.832347491282228,\n",
       " 10.32056275860947,\n",
       " 0.9900102872811674,\n",
       " -0.6663895416092592,\n",
       " 0.7602406083462949,\n",
       " 1.5994577091345228,\n",
       " -2.032701689205177,\n",
       " 1.3959053733321174,\n",
       " -0.27138491043147844,\n",
       " -0.23637931900362702,\n",
       " 1.7186793725054619,\n",
       " -1.160629446954734,\n",
       " -0.3068371509533023,\n",
       " -0.39454016920177654,\n",
       " 1.2068405720427444,\n",
       " -1.3345677895744303,\n",
       " -0.23562501808115485,\n",
       " -0.35125597715921936,\n",
       " -3.7676110888095278,\n",
       " 1.3115238859698555,\n",
       " 3.983910875688177,\n",
       " -1.7326222793865578,\n",
       " 1.4384412055236007,\n",
       " 0.2362189501824885,\n",
       " -1.091679269293247,\n",
       " -1.7427098208252585,\n",
       " 0.9987722300806756,\n",
       " 4.442734818386075,\n",
       " -0.30254045033981425,\n",
       " 0.4233762199749526,\n",
       " 1.6818133009091902,\n",
       " 0.3119570332321774,\n",
       " -1.5131441367665517,\n",
       " 2.799729240403045,\n",
       " 0.43980045707699894,\n",
       " -0.60693709441564,\n",
       " 1.725144883521973,\n",
       " 0.3371001936622006,\n",
       " -2.3473136236374055,\n",
       " 0.6382813881599075,\n",
       " -2.4107114684877393,\n",
       " -2.769478613971201,\n",
       " -3.0449054462872662,\n",
       " -0.8007757010270435,\n",
       " -1.6500008007780282,\n",
       " 0.2224932097073009,\n",
       " 3.05487042107651,\n",
       " 4.796790225277145,\n",
       " -1.7639752376913242,\n",
       " -0.5999128473178232,\n",
       " -0.8147260886199124,\n",
       " -0.8676530758199565,\n",
       " -2.7477699587504456,\n",
       " 5.224479592104522,\n",
       " 1.143761391080897,\n",
       " 0.49285292554944343,\n",
       " 0.7325625528738137,\n",
       " 1.5040661947288276,\n",
       " 0.12627679456132707,\n",
       " 0.37862928471541635,\n",
       " -1.3174100216769626,\n",
       " 2.0282779892591156,\n",
       " -0.9393473614924996,\n",
       " 1.786324457354659,\n",
       " 1.508280973108076,\n",
       " -1.6227539598856129,\n",
       " 0.03545944474559164,\n",
       " -0.2923509821287685,\n",
       " -1.7012871334186181,\n",
       " 1.074764357723848,\n",
       " -3.5496178249385792,\n",
       " -2.2323236654902923,\n",
       " -0.22547330280713998,\n",
       " 1.1500414909300285,\n",
       " 0.48024949222025287,\n",
       " 0.11623827171366216,\n",
       " 4.25487582210223,\n",
       " 1.4918968729746345,\n",
       " 5.377943892292932,\n",
       " -1.2688246667504315,\n",
       " 0.43236054399838836,\n",
       " 0.34731752271703387,\n",
       " 0.27266832544273745,\n",
       " 0.21717792923694226,\n",
       " 1.1160103107600223,\n",
       " 0.9044141633679268,\n",
       " -0.4795353761277257,\n",
       " 1.937681211217984,\n",
       " -2.1512768723955165,\n",
       " 0.4903704532839881,\n",
       " -0.849871581111976,\n",
       " 1.3720630242163467,\n",
       " 0.6809559933663326,\n",
       " 1.556786108842248,\n",
       " 0.4828469086777509,\n",
       " 2.009313606175614,\n",
       " 0.24904579234275354,\n",
       " 0.3485538447692202,\n",
       " 1.0942908173918542,\n",
       " -0.18176588194532634,\n",
       " 0.09214408168426348,\n",
       " 0.7420752809767749,\n",
       " 7.243777899842938,\n",
       " 0.49164466795524375,\n",
       " 0.7239016422456558,\n",
       " -2.8095875484103767,\n",
       " 4.349390659808734,\n",
       " 1.4890120001157285,\n",
       " 1.6284651366358092,\n",
       " -2.138393687526076,\n",
       " 0.047656012819675624,\n",
       " -0.8809900600485153,\n",
       " 0.322711968626016,\n",
       " 0.5502289950692552,\n",
       " 0.057534086046371335,\n",
       " 0.7043223405623337,\n",
       " -1.4269828857886542,\n",
       " 0.2374226376874266,\n",
       " -2.6934221101505313,\n",
       " 0.17652244839700537,\n",
       " 2.7166949202535124,\n",
       " 1.066285115827426,\n",
       " 0.9902349486822917,\n",
       " -1.733357930736028,\n",
       " 0.7754528227748495,\n",
       " -1.8664920806645269,\n",
       " 1.499080869009576,\n",
       " -0.5779007427462073,\n",
       " -1.00877281666807,\n",
       " 0.9484177049685323,\n",
       " 0.6123792182650359,\n",
       " -2.300099944523148,\n",
       " 1.0772237714192194,\n",
       " 3.4204888378679503,\n",
       " 0.8207871649209028,\n",
       " -0.9609571541714679,\n",
       " 0.865551231971903,\n",
       " 2.097381529005323,\n",
       " 0.7635036726112533,\n",
       " 1.7195198628329122,\n",
       " 5.732849037989765,\n",
       " -0.27406864101038436,\n",
       " 1.1676609364925992,\n",
       " 0.421113965987733,\n",
       " 0.33400174555654305,\n",
       " 0.20779705073566235,\n",
       " 1.8393398454225012,\n",
       " 1.4430464752292373,\n",
       " 0.6975908735062776,\n",
       " 0.4266183945242074,\n",
       " 0.37172329782899294,\n",
       " -0.7279369958756391,\n",
       " -0.11390665575682382,\n",
       " -2.5627047596528105,\n",
       " -1.5882390995057394,\n",
       " 0.04261074222549961,\n",
       " -0.33445076765844206,\n",
       " -3.1062230104975566,\n",
       " -1.7735065674094568,\n",
       " -0.6021875083298872,\n",
       " -3.0720939116222237,\n",
       " -3.472776543300418,\n",
       " -0.025341716838198185,\n",
       " -0.979978318763998,\n",
       " 2.9970795039366367,\n",
       " 0.8799878459965371,\n",
       " -0.25621313593570333,\n",
       " 5.8215564337935035,\n",
       " 0.6818222145551478,\n",
       " 0.6226176447709821,\n",
       " 0.38881808605555224,\n",
       " 0.455195091288494,\n",
       " -0.5777088781100215,\n",
       " -0.6799038594169031,\n",
       " -0.009346096474262086,\n",
       " 0.012853294461025372,\n",
       " 1.7767931612161503,\n",
       " -0.17900177328337563,\n",
       " 0.9015299358943338,\n",
       " 3.39610821277614,\n",
       " -2.0347117622064177,\n",
       " -2.331399329823769,\n",
       " 0.7914228156920695,\n",
       " -0.02576913791835686,\n",
       " -0.951446370208852,\n",
       " -0.08962358701053041,\n",
       " 0.4892609040253791,\n",
       " 0.6245158466025191,\n",
       " 0.20872509845668352,\n",
       " 0.18074403001099595,\n",
       " 0.46870498539813554,\n",
       " 0.06167921929747182,\n",
       " 0.778431607826688,\n",
       " 1.0099469771739178,\n",
       " -1.2155466386852802,\n",
       " 1.0406049807903672,\n",
       " -2.6467819018855683,\n",
       " 0.5484474381450155,\n",
       " 0.6467268946669263,\n",
       " 0.7867427790249916,\n",
       " 1.5106232546576435,\n",
       " -0.6927203413054039,\n",
       " 1.3404631364793242,\n",
       " 1.004876892193628,\n",
       " 3.5964371905523445,\n",
       " -1.2461369470533052,\n",
       " 0.3068670688438839,\n",
       " 1.49197414882963,\n",
       " 0.8581816645302057,\n",
       " 0.4225793058320658,\n",
       " -0.017762009076449745,\n",
       " -1.1278759258861824,\n",
       " 3.5888712217993,\n",
       " 1.4122474407403072,\n",
       " 0.9925524471159988,\n",
       " 1.0648958382387508,\n",
       " 0.8971793371625728,\n",
       " 0.4590776210658944,\n",
       " -1.62081589314457,\n",
       " 1.167370302121159,\n",
       " -2.8731409369398304,\n",
       " -1.5188151875645592,\n",
       " -0.4236411348107156,\n",
       " -1.6192886193588831,\n",
       " 0.5559688796306015,\n",
       " 0.07451692749414596,\n",
       " 0.34795184138822555,\n",
       " -0.3620878779689064,\n",
       " -0.2517034404804858,\n",
       " -1.5749485574446467,\n",
       " 0.24026969877494928,\n",
       " 2.579516077361434,\n",
       " -2.432589308166752,\n",
       " 0.6075863821078077,\n",
       " -2.207707118163371,\n",
       " -0.21908470078733538,\n",
       " -0.7782342159071407,\n",
       " -3.6974673168207097,\n",
       " 0.9755707102415258,\n",
       " -1.6014000438837992,\n",
       " 1.5730021936452494,\n",
       " -0.512349658371441,\n",
       " -0.21034671441114483,\n",
       " 2.09606678985839,\n",
       " 2.4024474793304846,\n",
       " 0.5835750715895546,\n",
       " 0.5170058619246909,\n",
       " 1.958729157837058,\n",
       " -0.6397792952004233,\n",
       " 34.09604838449306,\n",
       " 0.2769090536892071,\n",
       " 0.37088662019796875,\n",
       " -0.691579912877486,\n",
       " -0.02752391279192501,\n",
       " -0.7257940269850802,\n",
       " 1.8578405458930995,\n",
       " -1.1979725944354382,\n",
       " 1.4386135406927947,\n",
       " 1.593477237542847,\n",
       " 2.0297724006640996,\n",
       " -1.2551392603250875,\n",
       " 4.639374714372919,\n",
       " 0.609727163818315,\n",
       " -0.4379725541710776,\n",
       " -0.9796994765384199,\n",
       " -0.21873355995907703,\n",
       " 0.7455522791959367,\n",
       " -1.24711037486551,\n",
       " 0.3758515254929313,\n",
       " -0.4293434768693802,\n",
       " -0.6156908467615442,\n",
       " -0.34667729288103005,\n",
       " -1.6456140618442927,\n",
       " 1.422891366974906,\n",
       " -0.7467467187454129,\n",
       " -3.0988649429625386,\n",
       " 0.7949269514951611,\n",
       " -0.40000797737248117,\n",
       " 0.5695563986934251,\n",
       " -1.1933666431647705,\n",
       " -1.1673700095327848,\n",
       " 1.3959359897446606,\n",
       " 0.7653788996798716,\n",
       " -0.5408879946475045,\n",
       " -0.4616199109418211,\n",
       " -0.3762675294678448,\n",
       " -1.9409595520156926,\n",
       " -3.153180467652092,\n",
       " -0.8537152724550536,\n",
       " 1.3142126226192126,\n",
       " 1.0955609905876915,\n",
       " -0.7597958613228732,\n",
       " 14.199886678698512,\n",
       " 10.999280993740358,\n",
       " 0.6864462636453594,\n",
       " 2.7442375544224573,\n",
       " -1.2702577478375199,\n",
       " -0.6689513816340102,\n",
       " 0.3750408133676364,\n",
       " -0.3697596192455811,\n",
       " -1.3813235141619746,\n",
       " -0.799621367922108,\n",
       " -1.0646126478522844,\n",
       " 0.820743229989832,\n",
       " -2.1609516902284462,\n",
       " -2.472089685998247,\n",
       " 0.4680235231876822,\n",
       " 0.8470532548497888,\n",
       " 5.14430856836978,\n",
       " -0.472423232121812,\n",
       " 1.0379174012085006,\n",
       " -0.16387048593431586,\n",
       " 1.1925800687095238,\n",
       " 1.6018782311416957,\n",
       " -0.6037893363653091,\n",
       " -0.4321974929772926,\n",
       " 1.6967835719332707,\n",
       " 0.6583784480291136,\n",
       " 0.5275033064234618,\n",
       " 0.5247699365986436,\n",
       " -0.3240201593633917,\n",
       " -1.7918873714106023,\n",
       " 0.6617540619131117,\n",
       " -3.267296989370724,\n",
       " -0.6620784505854928,\n",
       " 2.4412280816927767,\n",
       " 3.4531725054131,\n",
       " 3.8193921591540425,\n",
       " 1.4346674388408307,\n",
       " -0.1610493194380816,\n",
       " -0.19321799243513246,\n",
       " 0.846231967546153,\n",
       " 0.2903826127061449,\n",
       " 0.3530664501426366,\n",
       " 1.3463087920618908,\n",
       " 1.1823081114977292,\n",
       " 1.4393071062118248,\n",
       " 1.8855559115241027,\n",
       " 1.7421687531637564,\n",
       " 1.3411042540965479,\n",
       " 1.4446831211807116,\n",
       " 2.5417089592042323,\n",
       " 1.0783905573863781,\n",
       " 1.9802805638261975,\n",
       " 2.250861181390629,\n",
       " 3.6137741107186963,\n",
       " -0.6549652942166249,\n",
       " 1.4873792755832147,\n",
       " 0.45318285564740846,\n",
       " 4.908172238123091,\n",
       " 4.193784789193579,\n",
       " -1.9646566627179074,\n",
       " -2.4339166695757326,\n",
       " -0.8828578620673411,\n",
       " -1.0439761868348827,\n",
       " 3.100398916801432,\n",
       " 2.1246981243773737]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc2.predict(norm_Trx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
